---
layout: default
---
<body>
    <section class="main-content">
      <h1 id="cs7641-project" align="center">CS7641 - Project Team 17</h1>

<h2 id="introductionbackground">Introduction</h2>
<h3 id="VQA">Visual Question Answering for Visually Impaired
</h3>  
        
<p>To resolve the visual challenges faced by the visually impaired people in their day-to-day lives, we present a Machine Learning model based on the Vizwiz dataset. The training dataset consists of photos taken by the blind people annotated with the question asked relevant to that image. Each annotation question  also consists of answers and answer types specified by 10 people for each sample. This provides an opportunity as well a challenge to assist the visually impaired to help them in navigation, assisting their daily life tasks and answering their visual questions etc. The original Vizwiz dataset consisted of :
</p>
<li>20,523 training image/question pairs</li>
<li>4,319 validation image/question pairs</li>
<li>8,000 test image/question pairs</li>
 
<h2 id="Step 1">Step 1. Data Preprocessing</h2>
        
<h3> Feature Extraction using Convolutional Neural Networks </h3>       
        
<p>We use transfer learning to create feature vectors for the images present in the dataset. The activations from the last layer of different 3 pre-trained models like Inceptionv3, ResNet (Residual Network) and VGG16 (Very Deep Convolutional Networks for Large-Scale Image Recognition), which are state-of-the-art and are widely used. These models are pre-trained on ImageNet, which is a huge image dataset containing more than 14 million images. Hence, they can be used for our task to create feature vectors of the images considered for our task from the Vizwiz dataset.
</p>       
        
<p> We first ran the Inceptionv3 using CPU and GPU, with and without batching. The time taken for each of these operations are displayed in Table . 
</p>

<table>
    
</table>

<h2 id="Clustering">Unsupervised Algorithm:  Clustering </h2>
    
<p>On closely examining the dataset, we observed that few images are totally blurred, few are black/white and few others just have too much flash. Hence we realised the need to clean the dataset before feeding it to our training pipeline. 
</p>

<h4>Challenge 1: </h4>
        <p>The first challenge that we faced is how to identify the images to be discarded.</p>
        
<h4>Solution:</h4>
<li>To identify such groups of images, we decided to perform K-means clustering on the original image dataset to generate similar clusters.</li>
</li>After generating clusters of similar images, we manually went through each of the clusters, and discarded the clusters that seemed useless for our use case.</li>
<li>The discarded clusters include clusters having blurred/ unclear images.</li>

<h4>Challenge 2: </h4>    
        <p>K-means clustering uses Euclidean distance as the metric to determine the similarity between different images. But some images might contain similar objects but they might be present in different orientations/positions/color contrasts, etc. Hence considering just the euclidean distance between the pixel values is not a good metric. 
        </p>

<h4>Solution:</h4>

<p>We used CNN to detect and classify the objects and accordingly performed the clustering on these CNN generated feature vectors.
Each CNN layer performs optimized operations like centering the objects, gray-scale conversion(normalization), detecting the objects, and accordingly classifying and clustering the images.
</p>
    
<h4>Challenge 3: </h4>   
<p>The next challenge that we faced was to identify the correct value of K(number of clusters) to be generated.</p>

<h4>Solution:</h4>

<p>To identify the accurate number of clusters to be considered, we calculated the loss values for a variety of distinct K values.
We then plotted each of these values and used the Elbow method to determine the best value of K to be considered for our use case.</p>

<p>The following table depicts the loss values obtained for each value of k:</p>

<table>
    
</table>

<p>The following is the elbow curve we generated based on the above values:</p>

<img></img>

<p>Hence, based on the elbow method, we obtained the accurate value of K as 60. Therefore, we generated 60 clusters. We then manually visited each of the clusters(i.e. each of the 60 folders of images), and discarded a few clusters to create a clean image dataset.

In this way, the dataset was reduced from 20,523 image/question pairs to 13000 image/question pairs.
</p>

<p>The following images show samples that were discarded:</p>
<img></img>

<p>The following images show samples that were included:</p>
<img></img>


<h4>Results of Data preprocessing</h4>
<p>To overcome challenges 1,2 and 3 we ran K-means on feature vectors generated by a forward pass of a Convolutional Neural Network to generate 60 clusters, and out of those 60 clusters, we kept 29 clusters and discarded the rest based on the reasoning provided before, thus generating a dataset consisting of 13000 image/question pairs. 
</p>

<h4>Annotation Script</h4>

<p>After generating 29 folders(clusters), we had to use only the images from these folders. To uniquely read images from 29 out of 60 folders, we wrote an annotation script.
</p>
<p>The script reads the annotation json file, which consists of the following structure:</p>

<pre class="line-numbers">
   <code class="language-css">
{
Image_name : “ “, // Image name
“Question: “ “, // The question associated with each image
“Answers” : {
		// List of answers from 10 people
	      }
“Answer_type”:[ yes/no], [other], [unanswerable],
“Answerable” : 0/1
}
  </code>
</pre>

<p>From this file, only those images will be considered that are present in the 29 clusters, and for those image names, we read questions and answers associated with them. All this data is then loaded into a new json file.
</p>
<p>This new json file is then used for further processing.</p>
</p>

<h3>Average and Max Pooling</h3>

<p>We also experimented taking max pooling and average pooling post the convolution layers, the results of which are shared in Table.
</p>

<h3>Resnet V/s Inception V/s VGG16</h3>
<h4>InceptionV3_Max_Normalized</h4>

<p>Loss Values:  [7691.446390113959, 7507.774507255225, 7391.517725536679, 7226.133910619914, 6991.397552682225, 6746.565646699269, 6499.366045281326, 6356.497762813506, 5795.658596714202, 5321.662363185365]
</p>

<table>
</table>

<img>
</img>

<h4>InceptionV3_Avg_Normalized</h4>

<table>
</table>

<img>
</img>

<h4>InceptionV3_Max_Unnormalized</h4>

<table>
</table>

<img>
</img>

<h4>InceptionV3_Avg_Unnormalized</h4>

<table>
</table>

<img>
</img>

<h4>ResNet50_Max_Normalized</h4>

<table>
</table>

<img>
</img>

<p>We finally use Inceptionv3 with max-pooling and run it on GPU using 32-sized batches. K-Means is run on these image feature vectors and based on the loss values received, we selected this architecture. 
</p>

</body>
    
