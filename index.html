---
layout: default
---
<body>
    <section class="main-content">
      <h1 id="cs7641-project" align="center">CS7641 - Project Team 17</h1>

<h2 id="introductionbackground">Introduction/Background</h2>
<h3 id="VQA">Visual Question Answering for Visually Impaired
</h3>  
        
<p>To resolve the visual challenges faced by the visually impaired people in their day-to-day lives, we present a Machine Learning model based on the Vizwiz dataset. The training dataset consists of photos taken by the blind people annotated with the question asked relevant to that image. Each annotation question  also consists of answers and answer types specified by 10 people for each sample. This provides an opportunity as well a challenge to assist the visually impaired to help them in navigation, assisting their daily life tasks and answering their visual questions etc. The original Vizwiz dataset consisted of :
</p>
<li>20,523 training image/question pairs</li>
<li>4,319 validation image/question pairs</li>
<li>8,000 test image/question pairs</li>
 
<h2 id="Step 1">Step 1. Data Preprocessing</h2>
        
<h3> Feature Extraction using Convolutional Neural Networks </h3>       
        
<p>We use transfer learning to create feature vectors for the images present in the dataset. The activations from the last layer of different 3 pre-trained models like Inceptionv3, ResNet (Residual Network) and VGG16 (Very Deep Convolutional Networks for Large-Scale Image Recognition), which are state-of-the-art and are widely used. These models are pre-trained on ImageNet, which is a huge image dataset containing more than 14 million images. Hence, they can be used for our task to create feature vectors of the images considered for our task from the Vizwiz dataset.
</p>       
        
<p> We first ran the Inceptionv3 using CPU and GPU, with and without batching. The time taken for each of these operations are displayed in Table . 
</p>

<table>
    
</table>

<h3 id="datasets">Datasets</h3>
<ul>
<li>Object Detection dataset - Consists of images and object categories that the images encompass.
</li>
<li>Questions and Answers dataset - Consists of text based questions. A query will be a question that the visually impaired person will ask to the system, and an answer would be the output of the model. </li>
</li>
<li>RefCOCO, Flickr30k, CLEVR
</li>
</ul>
        
<h3 id="machine_learning_checkpoints">Machine Learning Checkpoints</h3>
<ul>
<li>Using supervised algorithms such as CNN, we will build the object detection model to detect objects in the surroundings 
</li>
<li>Using unsupervised and semi-supervised algorithms such as vanilla-RNN, LSTM, GRU, we will develop a text detection system to understand the query input. 
</li>
<li>We will later build a multi-modal model combining the above models to achieve our goal. We will then perform a comparison of the results of our model with existing state-of-the-art (SOTA) models. 
</li>
</ul>
    
<p>So when integrated into an e-stick, the visually impaired will be providing a speech input to the system, which will be converted into text. Our model will use this text as an input and provide an answer to the query in text, which can then again be converted into a voice-controlled output for the visually impaired to understand.
</p>
    
<h2 id="scope">Project Scope</h2>
<u1>
<li>Speech-to-text or text-to-speech conversions modules will not be included in the project.
</li>
<li>Nature of user input queries will be limited to the predefined dataset.
</li>
</ul>
    
<h2 id="potential_results">Project Results</h2>
<u1>
<li>We aim to develop a system that can potentially make the navigation for a section of population a little better. 
</li>
<li>We aim to build our model by comparing the results of the State of the Art models available today as a benchmark. 
</li>
<li>We foresee the potential of further use of this model for various other navigation applications such as using the developed framework for visual-language navigation for mobile robots.</p>
</li>
</u1>

<h2 id="timeline">Proposed Timeline</h2>
<p><img src="Planning.png" alt="image" style=”display:block;”></p>

<h2 id="responsibilities">Responsibilities</h2>
<p><img src="Roles slide.png" alt="image" style=”display:block;” align="cnetre"></p>

<h2 id="references">References</h2>
<ul>
<li>Kunja Bihari Swain et. al. Arduino based automated stick guide for a visually impaired person, S.A. Engineering College, Chennai, Tamil Nadu, India, 2014.
</li>
<li>Ashwini B Yadav, et. al., Design and Development of Smart Assistive Device for Visually Impaired People, IEEE International Conference On Recent Trends In Electronics Information Communication Technology, India, May 2016. 
</li>
<li>Zeeshan Saquib, et. al.BlinDar: An Invisible Eye for the Blind People, IEEE International Conference On Recent Trends In Electronics Information Communication Technology, India, May 2017. 
</li>
<li>Ayat A. Nada et. al.Assistive Infrared Sensor Based Smart Stick for Blind People, Science and Information Conference, London, UK, 2015. 
</li>
<li>Nishant Banat James, Ashish Harsola. Navigation aiding stick for visually impaired, International Conference on Green Computing and Internet of Things, May 2015. 
</li>
<li>Sunil Kanzariya, Prof. Vishal Vora, “Real Time Video Monitoring System Using Raspberry Pi”, National Conference on Emerging Trends in Computer, Electrical Electronics, 2015. 
</li>
<li>Lun Zhang, Stan Z. Li, Xiaotong Yuan and Shiming Xiang, “Real-time Object Classification in Video Surveillance Based on Appearance Learning”, IEEE Conference on Computer Vision and Pattern Recognition,2007. 
</li>
</ul>
    
       
</section>
</body>
    
